{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63536b42-8aeb-47b4-be89-09943923fbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import CausalSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0b5de7b-507b-4203-9d2a-d933396bf239",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import scaled_dot_product_attention\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57e06dbd-7333-477d-8696-105e8b464f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dboyliao/Work/Sciwork/SciConf_2024/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.fx as fx\n",
    "import torch.onnx as onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d975ce2b-b389-4ffa-8402-bbfcab6bcf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_module = CausalSelfAttention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74c154e-4d70-409c-b465-f6b051d7755b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff2af1ba-9a7d-4a16-bf3b-33157a9fddfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_x = torch.randn((1, att_module.config.block_size, att_module.config.n_embd)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a195a98b-7e44-4004-a9d8-4603d942675d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dboyliao/Work/Sciwork/SciConf_2024/.venv/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:638: UserWarning: Was not able to add assertion to guarantee correct input x to specialized function. It is up to the user to make sure that your inputs match the inputs you specialized the function with.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "gm = fx.symbolic_trace(att_module, concrete_args={\"x\": sample_x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2adaa2c8-14a4-42a1-9cbf-b1acccc3b0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name               target                             args                            kwargs\n",
      "-------------  -----------------  ---------------------------------  ------------------------------  --------------------------------------------\n",
      "placeholder    x_1                x_1                                ()                              {}\n",
      "get_attr       _tensor_constant0  _tensor_constant0                  ()                              {}\n",
      "call_module    c_attn             c_attn                             (_tensor_constant0,)            {}\n",
      "call_method    split              split                              (c_attn, 768)                   {'dim': 2}\n",
      "call_function  getitem            <built-in function getitem>        (split, 0)                      {}\n",
      "call_function  getitem_1          <built-in function getitem>        (split, 1)                      {}\n",
      "call_function  getitem_2          <built-in function getitem>        (split, 2)                      {}\n",
      "call_method    view               view                               (getitem_1, 1, 1024, 12, 64)    {}\n",
      "call_method    transpose          transpose                          (view, 1, 2)                    {}\n",
      "call_method    view_1             view                               (getitem, 1, 1024, 12, 64)      {}\n",
      "call_method    transpose_1        transpose                          (view_1, 1, 2)                  {}\n",
      "call_method    view_2             view                               (getitem_2, 1, 1024, 12, 64)    {}\n",
      "call_method    transpose_2        transpose                          (view_2, 1, 2)                  {}\n",
      "call_method    transpose_3        transpose                          (transpose, -2, -1)             {}\n",
      "call_function  matmul             <built-in function matmul>         (transpose_1, transpose_3)      {}\n",
      "call_method    size               size                               (transpose, -1)                 {}\n",
      "call_function  sqrt               <built-in function sqrt>           (size,)                         {}\n",
      "call_function  truediv            <built-in function truediv>        (1.0, sqrt)                     {}\n",
      "call_function  mul                <built-in function mul>            (matmul, truediv)               {}\n",
      "get_attr       _tensor_constant4  _tensor_constant4                  ()                              {}\n",
      "call_method    masked_fill        masked_fill                        (mul, _tensor_constant4, -inf)  {}\n",
      "call_function  softmax            <function softmax at 0x1094ce710>  (masked_fill,)                  {'dim': -1, '_stacklevel': 3, 'dtype': None}\n",
      "call_module    attn_dropout       attn_dropout                       (softmax,)                      {}\n",
      "call_function  matmul_1           <built-in function matmul>         (attn_dropout, transpose_2)     {}\n",
      "call_method    transpose_4        transpose                          (matmul_1, 1, 2)                {}\n",
      "call_method    contiguous         contiguous                         (transpose_4,)                  {}\n",
      "call_method    view_3             view                               (contiguous, 1, 1024, 768)      {}\n",
      "call_module    c_proj             c_proj                             (view_3,)                       {}\n",
      "call_module    resid_dropout      resid_dropout                      (c_proj,)                       {}\n",
      "output         output             output                             (resid_dropout,)                {}\n"
     ]
    }
   ],
   "source": [
    "gm.graph.print_tabular()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9602308d-a8e2-4fd2-96e8-bb490b404992",
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in gm.graph.nodes:\n",
    "    if node.op == \"call_module\":\n",
    "        target = node.target\n",
    "        sub_module = getattr(gm, target)\n",
    "        if not isinstance(sub_module, nn.Dropout):\n",
    "            continue\n",
    "        node.kwargs = {\"p\": sub_module.p, \"training\": sub_module.training, \"inplace\": sub_module.inplace}\n",
    "        node.target = F.dropout\n",
    "        node.op = \"call_function\"\n",
    "_ = gm.recompile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a7f8a38c-24d8-4abd-af7d-bc532c7fad63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name               target                             args                            kwargs\n",
      "-------------  -----------------  ---------------------------------  ------------------------------  ----------------------------------------------\n",
      "placeholder    x_1                x_1                                ()                              {}\n",
      "get_attr       _tensor_constant0  _tensor_constant0                  ()                              {}\n",
      "call_module    c_attn             c_attn                             (_tensor_constant0,)            {}\n",
      "call_method    split              split                              (c_attn, 768)                   {'dim': 2}\n",
      "call_function  getitem            <built-in function getitem>        (split, 0)                      {}\n",
      "call_function  getitem_1          <built-in function getitem>        (split, 1)                      {}\n",
      "call_function  getitem_2          <built-in function getitem>        (split, 2)                      {}\n",
      "call_method    view               view                               (getitem_1, 1, 1024, 12, 64)    {}\n",
      "call_method    transpose          transpose                          (view, 1, 2)                    {}\n",
      "call_method    view_1             view                               (getitem, 1, 1024, 12, 64)      {}\n",
      "call_method    transpose_1        transpose                          (view_1, 1, 2)                  {}\n",
      "call_method    view_2             view                               (getitem_2, 1, 1024, 12, 64)    {}\n",
      "call_method    transpose_2        transpose                          (view_2, 1, 2)                  {}\n",
      "call_method    transpose_3        transpose                          (transpose, -2, -1)             {}\n",
      "call_function  matmul             <built-in function matmul>         (transpose_1, transpose_3)      {}\n",
      "call_method    size               size                               (transpose, -1)                 {}\n",
      "call_function  sqrt               <built-in function sqrt>           (size,)                         {}\n",
      "call_function  truediv            <built-in function truediv>        (1.0, sqrt)                     {}\n",
      "call_function  mul                <built-in function mul>            (matmul, truediv)               {}\n",
      "get_attr       _tensor_constant4  _tensor_constant4                  ()                              {}\n",
      "call_method    masked_fill        masked_fill                        (mul, _tensor_constant4, -inf)  {}\n",
      "call_function  softmax            <function softmax at 0x1094ce710>  (masked_fill,)                  {'dim': -1, '_stacklevel': 3, 'dtype': None}\n",
      "call_function  attn_dropout       <function dropout at 0x1094cdbd0>  (softmax,)                      {'p': 0.0, 'training': True, 'inplace': False}\n",
      "call_function  matmul_1           <built-in function matmul>         (attn_dropout, transpose_2)     {}\n",
      "call_method    transpose_4        transpose                          (matmul_1, 1, 2)                {}\n",
      "call_method    contiguous         contiguous                         (transpose_4,)                  {}\n",
      "call_method    view_3             view                               (contiguous, 1, 1024, 768)      {}\n",
      "call_module    c_proj             c_proj                             (view_3,)                       {}\n",
      "call_function  resid_dropout      <function dropout at 0x1094cdbd0>  (c_proj,)                       {'p': 0.0, 'training': True, 'inplace': False}\n",
      "output         output             output                             (resid_dropout,)                {}\n"
     ]
    }
   ],
   "source": [
    "gm.graph.print_tabular()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "64db0ab3-7f2a-42f1-84b7-cad9af6b970a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-5.7285e-01,  5.5057e-01, -7.4936e-01,  ...,  5.2238e-04,\n",
       "          -3.5449e-01, -1.6398e-01],\n",
       "         [-1.2847e-01,  1.0453e-01, -3.9423e-02,  ..., -1.1701e-01,\n",
       "          -2.7683e-01, -9.2623e-02],\n",
       "         [-1.3511e-01, -2.8250e-02, -8.8111e-02,  ...,  1.2655e-01,\n",
       "          -3.9318e-02, -2.9731e-01],\n",
       "         ...,\n",
       "         [-4.6995e-02,  1.9405e-02, -5.9787e-03,  ...,  4.6384e-02,\n",
       "           2.2823e-02, -4.5394e-02],\n",
       "         [-4.1740e-02,  1.3565e-02, -1.4079e-02,  ...,  3.2467e-02,\n",
       "           2.4587e-02, -3.9794e-02],\n",
       "         [-3.1664e-02,  1.4018e-02, -1.6495e-02,  ...,  2.2946e-02,\n",
       "           1.7570e-02, -3.2156e-02]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm(sample_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8521f56d-39d6-4c8e-aee8-35610cbe8d9c",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ef88f63a-e9fc-440d-987b-6b612aae549d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replacement(q, k, v):\n",
    "    return scaled_dot_product_attention(\n",
    "        q, k, v, attn_mask=None, dropout_p=0.0, is_causal=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3b08cd0a-80d0-4670-b53b-0c3023e978d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = att_module.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2bac99ea-6901-4b06-a9d0-70a34bb9d702",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatternModule(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                        .view(1, 1, config.block_size, config.block_size))\n",
    "    def forward(self, q, k, v):\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:1024,:1024] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = F.dropout(att, p=0.0)\n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(1, 1024, 768)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e4268b51-4093-4311-8fa7-cbb99a59bad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name                          target                                            args                             kwargs\n",
      "-------------  ----------------------------  ------------------------------------------------  -------------------------------  --------------------------------------------------------\n",
      "placeholder    q                             q                                                 ()                               {}\n",
      "placeholder    k                             k                                                 ()                               {}\n",
      "placeholder    v                             v                                                 ()                               {}\n",
      "call_function  scaled_dot_product_attention  <built-in function scaled_dot_product_attention>  (q, k, v)                        {'attn_mask': None, 'dropout_p': 0.0, 'is_causal': True}\n",
      "output         output                        output                                            (scaled_dot_product_attention,)  {}\n"
     ]
    }
   ],
   "source": [
    "fx.symbolic_trace(replacement).graph.print_tabular()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4b632837-e9a1-4f6b-9476-3e2ec24a006c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern(q, k, v):\n",
    "    pm = PatternModule(att_module.config)\n",
    "    return pm.forward(q, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "45889d94-61cd-46dd-8a8b-ff0aea7d396e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name               target                             args                            kwargs\n",
      "-------------  -----------------  ---------------------------------  ------------------------------  ----------------------------------------------\n",
      "placeholder    q                  q                                  ()                              {}\n",
      "placeholder    k                  k                                  ()                              {}\n",
      "placeholder    v                  v                                  ()                              {}\n",
      "call_method    transpose          transpose                          (k, -2, -1)                     {}\n",
      "call_function  matmul             <built-in function matmul>         (q, transpose)                  {}\n",
      "call_method    size               size                               (k, -1)                         {}\n",
      "call_function  sqrt               <built-in function sqrt>           (size,)                         {}\n",
      "call_function  truediv            <built-in function truediv>        (1.0, sqrt)                     {}\n",
      "call_function  mul                <built-in function mul>            (matmul, truediv)               {}\n",
      "get_attr       _tensor_constant0  _tensor_constant0                  ()                              {}\n",
      "call_method    masked_fill        masked_fill                        (mul, _tensor_constant0, -inf)  {}\n",
      "call_function  softmax            <function softmax at 0x1094ce710>  (masked_fill,)                  {'dim': -1, '_stacklevel': 3, 'dtype': None}\n",
      "call_function  dropout            <function dropout at 0x1094cdbd0>  (softmax,)                      {'p': 0.0, 'training': True, 'inplace': False}\n",
      "call_function  matmul_1           <built-in function matmul>         (dropout, v)                    {}\n",
      "call_method    transpose_1        transpose                          (matmul_1, 1, 2)                {}\n",
      "call_method    contiguous         contiguous                         (transpose_1,)                  {}\n",
      "call_method    view               view                               (contiguous, 1, 1024, 768)      {}\n",
      "output         output             output                             (view,)                         {}\n"
     ]
    }
   ],
   "source": [
    "fx.symbolic_trace(pattern).graph.print_tabular()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d371b12d-4b03-4994-9f7e-6a90e806de36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Match(anchor=view, nodes_map={view: view_3, contiguous: contiguous, transpose_1: transpose_4, matmul_1: matmul_1, dropout: attn_dropout, softmax: softmax, masked_fill: masked_fill, mul: mul, matmul: matmul, q: transpose_1, transpose: transpose_3, k: transpose, truediv: truediv, sqrt: sqrt, size: size, _tensor_constant0: _tensor_constant4, v: transpose_2})]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fx.replace_pattern(gm, pattern=pattern, replacement=replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2ad42823-1709-4afe-989e-c99f3aee0294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name                          target                                            args                                   kwargs\n",
      "-------------  ----------------------------  ------------------------------------------------  -------------------------------------  --------------------------------------------------------\n",
      "placeholder    x_1                           x_1                                               ()                                     {}\n",
      "get_attr       _tensor_constant0             _tensor_constant0                                 ()                                     {}\n",
      "call_module    c_attn                        c_attn                                            (_tensor_constant0,)                   {}\n",
      "call_method    split                         split                                             (c_attn, 768)                          {'dim': 2}\n",
      "call_function  getitem                       <built-in function getitem>                       (split, 0)                             {}\n",
      "call_function  getitem_1                     <built-in function getitem>                       (split, 1)                             {}\n",
      "call_function  getitem_2                     <built-in function getitem>                       (split, 2)                             {}\n",
      "call_method    view                          view                                              (getitem_1, 1, 1024, 12, 64)           {}\n",
      "call_method    transpose                     transpose                                         (view, 1, 2)                           {}\n",
      "call_method    view_1                        view                                              (getitem, 1, 1024, 12, 64)             {}\n",
      "call_method    transpose_1                   transpose                                         (view_1, 1, 2)                         {}\n",
      "call_method    view_2                        view                                              (getitem_2, 1, 1024, 12, 64)           {}\n",
      "call_method    transpose_2                   transpose                                         (view_2, 1, 2)                         {}\n",
      "call_function  scaled_dot_product_attention  <built-in function scaled_dot_product_attention>  (transpose_1, transpose, transpose_2)  {'attn_mask': None, 'dropout_p': 0.0, 'is_causal': True}\n",
      "call_module    c_proj                        c_proj                                            (scaled_dot_product_attention,)        {}\n",
      "call_function  resid_dropout                 <function dropout at 0x1094cdbd0>                 (c_proj,)                              {'p': 0.0, 'training': True, 'inplace': False}\n",
      "output         output                        output                                            (resid_dropout,)                       {}\n"
     ]
    }
   ],
   "source": [
    "gm.graph.print_tabular()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d7262dd7-9508-4ea0-85ec-3750654536cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_dim, out_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return self.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "abd7642f-f87b-4548-ad76-92542abef1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(MLP(256, 128), MLP(128, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9a627671-e928-4c31-ad00-062966e394d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "494cdb03-72ad-4085-8327-aa8e77cf7116",
   "metadata": {},
   "outputs": [],
   "source": [
    "ep = export(model, (x,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "56361c20-e2ed-4c66-93b3-5782b709d83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_attr 0_linear_weight _0_linear_weight ['val', 'tensor_meta', 'stack_trace', 'nn_module_stack', 'source_fn_stack', 'from_node', 'seq_nr', 'example_value']\n",
      "get_attr 0_linear_bias _0_linear_bias ['val', 'tensor_meta', 'stack_trace', 'nn_module_stack', 'source_fn_stack', 'from_node', 'seq_nr', 'example_value']\n",
      "get_attr 1_linear_weight _1_linear_weight ['val', 'tensor_meta', 'stack_trace', 'nn_module_stack', 'source_fn_stack', 'from_node', 'seq_nr', 'example_value']\n",
      "get_attr 1_linear_bias _1_linear_bias ['val', 'tensor_meta', 'stack_trace', 'nn_module_stack', 'source_fn_stack', 'from_node', 'seq_nr', 'example_value']\n",
      "placeholder l_args_0_ l_args_0_ ['val', 'tensor_meta']\n",
      "call_function aten.t.default t ['stack_trace', 'nn_module_stack', 'source_fn_stack', 'original_aten', 'from_node', 'seq_nr', 'val', 'tensor_meta']\n",
      "call_function aten.addmm.default addmm ['stack_trace', 'nn_module_stack', 'source_fn_stack', 'original_aten', 'from_node', 'seq_nr', 'val', 'tensor_meta']\n",
      "call_function aten.relu.default relu ['stack_trace', 'nn_module_stack', 'source_fn_stack', 'original_aten', 'from_node', 'seq_nr', 'val', 'tensor_meta']\n",
      "call_function aten.t.default t_1 ['stack_trace', 'nn_module_stack', 'source_fn_stack', 'original_aten', 'from_node', 'seq_nr', 'val', 'tensor_meta']\n",
      "call_function aten.addmm.default addmm_1 ['stack_trace', 'nn_module_stack', 'source_fn_stack', 'original_aten', 'from_node', 'seq_nr', 'val', 'tensor_meta']\n",
      "call_function aten.relu.default relu_1 ['stack_trace', 'nn_module_stack', 'source_fn_stack', 'original_aten', 'from_node', 'seq_nr', 'val', 'tensor_meta']\n",
      "output output output_1 []\n"
     ]
    }
   ],
   "source": [
    "nodes_map = {}\n",
    "for node in ep.module().graph.nodes:\n",
    "    print(node.op, node.target, node.name, [k for k in node.meta])\n",
    "    nodes_map[node.name] = node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "94e222b4-f24b-40f3-99f1-108adf6a8dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stack_trace': '  File \"/Users/dboyliao/Work/Sciwork/SciConf_2024/.venv/lib/python3.10/site-packages/torch/_dynamo/external_utils.py\", line 17, in inner\\n    return fn(*args, **kwargs)\\n  File \"/Users/dboyliao/Work/Sciwork/SciConf_2024/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\\n    return forward_call(*args, **kwargs)\\n  File \"/var/folders/5l/jrch3z5x6jx6tcz33qkv1mgh0000gn/T/ipykernel_44342/1711239079.py\", line 8, in forward\\n    x = self.linear(x)\\n',\n",
       " 'nn_module_stack': {'L__self__': ('', torch.nn.modules.container.Sequential),\n",
       "  'fn': (\"L['fn']\", torch.nn.modules.container.Sequential),\n",
       "  'fn_0': (\"getattr(L['fn'], '0')\", __main__.MLP),\n",
       "  'getattr_L__fn_____0___linear': (\"getattr(L['fn'], '0').linear\",\n",
       "   torch.nn.modules.linear.Linear)},\n",
       " 'source_fn_stack': [('getattr_l__fn_____0___linear',\n",
       "   torch.nn.modules.linear.Linear)],\n",
       " 'original_aten': <OpOverload(op='aten.t', overload='default')>,\n",
       " 'from_node': [('x', 'getattr_L__fn_____0___linear')],\n",
       " 'seq_nr': 169,\n",
       " 'val': FakeTensor(..., size=(256, 128)),\n",
       " 'tensor_meta': TensorMetadata(shape=torch.Size([256, 128]), dtype=torch.float32, requires_grad=False, stride=(1, 256), memory_format=None, is_quantized=False, qparams={})}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes_map['t'].meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "64a2097a-89d4-43cf-bfa3-49aea8d75dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4436537e-4105-4d21-8af2-488577c9fdc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "GraphModule                              [1, 64]                   41,152\n",
       "==========================================================================================\n",
       "Total params: 41,152\n",
       "Trainable params: 41,152\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 2.63\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.16\n",
       "Estimated Total Size (MB): 0.17\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(ep.module(), x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7015f41-fcd4-47f5-9d71-bfe4cfb28011",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sciconf-2024",
   "language": "python",
   "name": "sciconf-2024"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
