{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63536b42-8aeb-47b4-be89-09943923fbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import CausalSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0b5de7b-507b-4203-9d2a-d933396bf239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.fx as fx\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn.functional import scaled_dot_product_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d975ce2b-b389-4ffa-8402-bbfcab6bcf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_module = CausalSelfAttention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff2af1ba-9a7d-4a16-bf3b-33157a9fddfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_x = torch.randn((1, att_module.config.block_size, att_module.config.n_embd)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a195a98b-7e44-4004-a9d8-4603d942675d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dboyliao/Work/Sciwork/SciConf_2024/.venv/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:638: UserWarning: Was not able to add assertion to guarantee correct input x to specialized function. It is up to the user to make sure that your inputs match the inputs you specialized the function with.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "gm = fx.symbolic_trace(att_module, concrete_args={\"x\": sample_x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2adaa2c8-14a4-42a1-9cbf-b1acccc3b0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name               target                             args                            kwargs\n",
      "-------------  -----------------  ---------------------------------  ------------------------------  --------------------------------------------\n",
      "placeholder    x_1                x_1                                ()                              {}\n",
      "get_attr       _tensor_constant0  _tensor_constant0                  ()                              {}\n",
      "call_module    c_attn             c_attn                             (_tensor_constant0,)            {}\n",
      "call_method    split              split                              (c_attn, 768)                   {'dim': 2}\n",
      "call_function  getitem            <built-in function getitem>        (split, 0)                      {}\n",
      "call_function  getitem_1          <built-in function getitem>        (split, 1)                      {}\n",
      "call_function  getitem_2          <built-in function getitem>        (split, 2)                      {}\n",
      "call_method    view               view                               (getitem_1, 1, 1024, 12, 64)    {}\n",
      "call_method    transpose          transpose                          (view, 1, 2)                    {}\n",
      "call_method    view_1             view                               (getitem, 1, 1024, 12, 64)      {}\n",
      "call_method    transpose_1        transpose                          (view_1, 1, 2)                  {}\n",
      "call_method    view_2             view                               (getitem_2, 1, 1024, 12, 64)    {}\n",
      "call_method    transpose_2        transpose                          (view_2, 1, 2)                  {}\n",
      "call_method    transpose_3        transpose                          (transpose, -2, -1)             {}\n",
      "call_function  matmul             <built-in function matmul>         (transpose_1, transpose_3)      {}\n",
      "call_method    size               size                               (transpose, -1)                 {}\n",
      "call_function  sqrt               <built-in function sqrt>           (size,)                         {}\n",
      "call_function  truediv            <built-in function truediv>        (1.0, sqrt)                     {}\n",
      "call_function  mul                <built-in function mul>            (matmul, truediv)               {}\n",
      "get_attr       _tensor_constant1  _tensor_constant1                  ()                              {}\n",
      "call_method    masked_fill        masked_fill                        (mul, _tensor_constant1, -inf)  {}\n",
      "call_function  softmax            <function softmax at 0x1111167a0>  (masked_fill,)                  {'dim': -1, '_stacklevel': 3, 'dtype': None}\n",
      "call_module    attn_dropout       attn_dropout                       (softmax,)                      {}\n",
      "call_function  matmul_1           <built-in function matmul>         (attn_dropout, transpose_2)     {}\n",
      "call_method    transpose_4        transpose                          (matmul_1, 1, 2)                {}\n",
      "call_method    contiguous         contiguous                         (transpose_4,)                  {}\n",
      "call_method    view_3             view                               (contiguous, 1, 1024, 768)      {}\n",
      "call_module    c_proj             c_proj                             (view_3,)                       {}\n",
      "call_module    resid_dropout      resid_dropout                      (c_proj,)                       {}\n",
      "output         output             output                             (resid_dropout,)                {}\n"
     ]
    }
   ],
   "source": [
    "gm.graph.print_tabular()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9602308d-a8e2-4fd2-96e8-bb490b404992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dropout(gm: fx.GraphModule):\n",
    "    \"\"\"\n",
    "    Find all `self.dropout(x)` and replace it with `F.dropout`\n",
    "    \"\"\"\n",
    "    gm = deepcopy(gm)\n",
    "    for node in gm.graph.nodes:\n",
    "        if node.op == \"call_module\":\n",
    "            target = node.target\n",
    "            sub_module = getattr(gm, target)\n",
    "            if not isinstance(sub_module, nn.Dropout):\n",
    "                continue\n",
    "            node.kwargs = {\"p\": sub_module.p, \"training\": sub_module.training, \"inplace\": sub_module.inplace}\n",
    "            node.target = F.dropout\n",
    "            node.op = \"call_function\"\n",
    "    _ = gm.recompile()\n",
    "    return gm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7f8a38c-24d8-4abd-af7d-bc532c7fad63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name               target                             args                            kwargs\n",
      "-------------  -----------------  ---------------------------------  ------------------------------  ----------------------------------------------\n",
      "placeholder    x_1                x_1                                ()                              {}\n",
      "get_attr       _tensor_constant0  _tensor_constant0                  ()                              {}\n",
      "call_module    c_attn             c_attn                             (_tensor_constant0,)            {}\n",
      "call_method    split              split                              (c_attn, 768)                   {'dim': 2}\n",
      "call_function  getitem            <built-in function getitem>        (split, 0)                      {}\n",
      "call_function  getitem_1          <built-in function getitem>        (split, 1)                      {}\n",
      "call_function  getitem_2          <built-in function getitem>        (split, 2)                      {}\n",
      "call_method    view               view                               (getitem_1, 1, 1024, 12, 64)    {}\n",
      "call_method    transpose          transpose                          (view, 1, 2)                    {}\n",
      "call_method    view_1             view                               (getitem, 1, 1024, 12, 64)      {}\n",
      "call_method    transpose_1        transpose                          (view_1, 1, 2)                  {}\n",
      "call_method    view_2             view                               (getitem_2, 1, 1024, 12, 64)    {}\n",
      "call_method    transpose_2        transpose                          (view_2, 1, 2)                  {}\n",
      "call_method    transpose_3        transpose                          (transpose, -2, -1)             {}\n",
      "call_function  matmul             <built-in function matmul>         (transpose_1, transpose_3)      {}\n",
      "call_method    size               size                               (transpose, -1)                 {}\n",
      "call_function  sqrt               <built-in function sqrt>           (size,)                         {}\n",
      "call_function  truediv            <built-in function truediv>        (1.0, sqrt)                     {}\n",
      "call_function  mul                <built-in function mul>            (matmul, truediv)               {}\n",
      "get_attr       _tensor_constant1  _tensor_constant1                  ()                              {}\n",
      "call_method    masked_fill        masked_fill                        (mul, _tensor_constant1, -inf)  {}\n",
      "call_function  softmax            <function softmax at 0x1111167a0>  (masked_fill,)                  {'dim': -1, '_stacklevel': 3, 'dtype': None}\n",
      "call_function  attn_dropout       <function dropout at 0x111115c60>  (softmax,)                      {'p': 0.0, 'training': True, 'inplace': False}\n",
      "call_function  matmul_1           <built-in function matmul>         (attn_dropout, transpose_2)     {}\n",
      "call_method    transpose_4        transpose                          (matmul_1, 1, 2)                {}\n",
      "call_method    contiguous         contiguous                         (transpose_4,)                  {}\n",
      "call_method    view_3             view                               (contiguous, 1, 1024, 768)      {}\n",
      "call_module    c_proj             c_proj                             (view_3,)                       {}\n",
      "call_function  resid_dropout      <function dropout at 0x111115c60>  (c_proj,)                       {'p': 0.0, 'training': True, 'inplace': False}\n",
      "output         output             output                             (resid_dropout,)                {}\n"
     ]
    }
   ],
   "source": [
    "gm = normalize_dropout(gm)\n",
    "gm.graph.print_tabular()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8521f56d-39d6-4c8e-aee8-35610cbe8d9c",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b08cd0a-80d0-4670-b53b-0c3023e978d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = att_module.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef88f63a-e9fc-440d-987b-6b612aae549d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replacement(q, k, v):\n",
    "    return scaled_dot_product_attention(\n",
    "        q, k, v, attn_mask=None, dropout_p=0.0, is_causal=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4268b51-4093-4311-8fa7-cbb99a59bad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name                          target                                            args                             kwargs\n",
      "-------------  ----------------------------  ------------------------------------------------  -------------------------------  --------------------------------------------------------\n",
      "placeholder    q                             q                                                 ()                               {}\n",
      "placeholder    k                             k                                                 ()                               {}\n",
      "placeholder    v                             v                                                 ()                               {}\n",
      "call_function  scaled_dot_product_attention  <built-in function scaled_dot_product_attention>  (q, k, v)                        {'attn_mask': None, 'dropout_p': 0.0, 'is_causal': True}\n",
      "output         output                        output                                            (scaled_dot_product_attention,)  {}\n"
     ]
    }
   ],
   "source": [
    "fx.symbolic_trace(replacement).graph.print_tabular()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bac99ea-6901-4b06-a9d0-70a34bb9d702",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatternModule(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                        .view(1, 1, config.block_size, config.block_size))\n",
    "    def forward(self, q, k, v):\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:1024,:1024] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = F.dropout(att, p=0.0)\n",
    "        return att @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b632837-e9a1-4f6b-9476-3e2ec24a006c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern(q, k, v):\n",
    "    pm = PatternModule(att_module.config)\n",
    "    return pm.forward(q, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45889d94-61cd-46dd-8a8b-ff0aea7d396e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name               target                             args                            kwargs\n",
      "-------------  -----------------  ---------------------------------  ------------------------------  ----------------------------------------------\n",
      "placeholder    q                  q                                  ()                              {}\n",
      "placeholder    k                  k                                  ()                              {}\n",
      "placeholder    v                  v                                  ()                              {}\n",
      "call_method    transpose          transpose                          (k, -2, -1)                     {}\n",
      "call_function  matmul             <built-in function matmul>         (q, transpose)                  {}\n",
      "call_method    size               size                               (k, -1)                         {}\n",
      "call_function  sqrt               <built-in function sqrt>           (size,)                         {}\n",
      "call_function  truediv            <built-in function truediv>        (1.0, sqrt)                     {}\n",
      "call_function  mul                <built-in function mul>            (matmul, truediv)               {}\n",
      "get_attr       _tensor_constant0  _tensor_constant0                  ()                              {}\n",
      "call_method    masked_fill        masked_fill                        (mul, _tensor_constant0, -inf)  {}\n",
      "call_function  softmax            <function softmax at 0x1111167a0>  (masked_fill,)                  {'dim': -1, '_stacklevel': 3, 'dtype': None}\n",
      "call_function  dropout            <function dropout at 0x111115c60>  (softmax,)                      {'p': 0.0, 'training': True, 'inplace': False}\n",
      "call_function  matmul_1           <built-in function matmul>         (dropout, v)                    {}\n",
      "output         output             output                             (matmul_1,)                     {}\n"
     ]
    }
   ],
   "source": [
    "fx.symbolic_trace(pattern).graph.print_tabular()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d371b12d-4b03-4994-9f7e-6a90e806de36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Match(anchor=matmul_1, nodes_map={matmul_1: matmul_1, dropout: attn_dropout, softmax: softmax, masked_fill: masked_fill, mul: mul, matmul: matmul, q: transpose_1, transpose: transpose_3, k: transpose, truediv: truediv, sqrt: sqrt, size: size, _tensor_constant0: _tensor_constant1, v: transpose_2})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fx.replace_pattern(gm, pattern=pattern, replacement=replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ad42823-1709-4afe-989e-c99f3aee0294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name                          target                                            args                                   kwargs\n",
      "-------------  ----------------------------  ------------------------------------------------  -------------------------------------  --------------------------------------------------------\n",
      "placeholder    x_1                           x_1                                               ()                                     {}\n",
      "get_attr       _tensor_constant0             _tensor_constant0                                 ()                                     {}\n",
      "call_module    c_attn                        c_attn                                            (_tensor_constant0,)                   {}\n",
      "call_method    split                         split                                             (c_attn, 768)                          {'dim': 2}\n",
      "call_function  getitem                       <built-in function getitem>                       (split, 0)                             {}\n",
      "call_function  getitem_1                     <built-in function getitem>                       (split, 1)                             {}\n",
      "call_function  getitem_2                     <built-in function getitem>                       (split, 2)                             {}\n",
      "call_method    view                          view                                              (getitem_1, 1, 1024, 12, 64)           {}\n",
      "call_method    transpose                     transpose                                         (view, 1, 2)                           {}\n",
      "call_method    view_1                        view                                              (getitem, 1, 1024, 12, 64)             {}\n",
      "call_method    transpose_1                   transpose                                         (view_1, 1, 2)                         {}\n",
      "call_method    view_2                        view                                              (getitem_2, 1, 1024, 12, 64)           {}\n",
      "call_method    transpose_2                   transpose                                         (view_2, 1, 2)                         {}\n",
      "call_function  scaled_dot_product_attention  <built-in function scaled_dot_product_attention>  (transpose_1, transpose, transpose_2)  {'attn_mask': None, 'dropout_p': 0.0, 'is_causal': True}\n",
      "call_method    transpose_4                   transpose                                         (scaled_dot_product_attention, 1, 2)   {}\n",
      "call_method    contiguous                    contiguous                                        (transpose_4,)                         {}\n",
      "call_method    view_3                        view                                              (contiguous, 1, 1024, 768)             {}\n",
      "call_module    c_proj                        c_proj                                            (view_3,)                              {}\n",
      "call_function  resid_dropout                 <function dropout at 0x111115c60>                 (c_proj,)                              {'p': 0.0, 'training': True, 'inplace': False}\n",
      "output         output                        output                                            (resid_dropout,)                       {}\n"
     ]
    }
   ],
   "source": [
    "gm.graph.print_tabular()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "beab343e-50fa-4d6b-884a-2b8bccf24980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.allclose(\n",
    "    att_module(sample_x).detach().cpu().numpy(),\n",
    "    gm(sample_x).detach().cpu().numpy(),\n",
    "    atol=1e-7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1e0b0d-c166-438c-91a3-e071f06c9de2",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sciconf-2024",
   "language": "python",
   "name": "sciconf-2024"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
