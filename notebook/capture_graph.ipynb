{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "816a4a0f-7b61-4c25-b643-b744602576aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85ed9e46-9923-4a4a-93ca-ea971ee0357b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9673002d-d44e-4d23-be99-55784eb66959",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46754702-def5-4d67-9055-d461b236527a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1c3769fd-69a4-474a-9bf0-a56968c2cf9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "744341b8-1b2d-4524-8d63-f2a6729ca4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config = None):\n",
    "        super().__init__()\n",
    "        if config is None:\n",
    "            config = GPTConfig()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.__config = config\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                        .view(1, 1, config.block_size, config.block_size))\n",
    "    @property\n",
    "    def config(self):\n",
    "        return self.__config\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        # manual implementation of attention\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = F.dropout(att, p=self.__config.dropout)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c0b5de7b-507b-4203-9d2a-d933396bf239",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import scaled_dot_product_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "803dca26-7769-49d6-a7ae-ede85664dcea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False, scale=None) -> Tensor:\n",
      "\n",
      "Computes scaled dot product attention on query, key and value tensors, using\n",
      "an optional attention mask if passed, and applying dropout if a probability\n",
      "greater than 0.0 is specified.\n",
      "\n",
      ".. code-block:: python\n",
      "\n",
      "    # Efficient implementation equivalent to the following:\n",
      "    def scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False, scale=None) -> torch.Tensor:\n",
      "        # Efficient implementation equivalent to the following:\n",
      "        L, S = query.size(-2), key.size(-2)\n",
      "        scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale\n",
      "        attn_bias = torch.zeros(L, S, dtype=query.dtype)\n",
      "        if is_causal:\n",
      "            assert attn_mask is None\n",
      "            temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0)\n",
      "            attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n",
      "            attn_bias.to(query.dtype)\n",
      "\n",
      "        if attn_mask is not None:\n",
      "            if attn_mask.dtype == torch.bool:\n",
      "                attn_bias.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\n",
      "            else:\n",
      "                attn_bias += attn_mask\n",
      "        attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
      "        attn_weight += attn_bias\n",
      "        attn_weight = torch.softmax(attn_weight, dim=-1)\n",
      "        attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n",
      "        return attn_weight @ value\n",
      "\n",
      ".. warning:: This function is beta and subject to change.\n",
      "\n",
      "Note:\n",
      "\n",
      "    There are currently three supported implementations of scaled dot product attention:\n",
      "\n",
      "        - `FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning`_\n",
      "        - `Memory-Efficient Attention`_\n",
      "        - A PyTorch implementation defined in C++ matching the above formulation\n",
      "\n",
      "    The function may call optimized kernels for improved performance when using the CUDA backend.\n",
      "    For all other backends, the PyTorch implementation will be used.\n",
      "\n",
      "    All implementations are enabled by default. Scaled dot product attention attempts to automatically select the\n",
      "    most optimal implementation based on the inputs. In order to provide more fine-grained control over what implementation\n",
      "    is used, the following functions are provided for enabling and disabling implementations.\n",
      "    The context manager is the preferred mechanism:\n",
      "\n",
      "        - :func:`torch.backends.cuda.sdp_kernel`: A context manager used to enable/disable any of the implementations.\n",
      "        - :func:`torch.backends.cuda.enable_flash_sdp`: Enables or Disables FlashAttention.\n",
      "        - :func:`torch.backends.cuda.enable_mem_efficient_sdp`: Enables or Disables Memory-Efficient Attention.\n",
      "        - :func:`torch.backends.cuda.enable_math_sdp`: Enables or Disables the PyTorch C++ implementation.\n",
      "\n",
      "    Each of the fused kernels has specific input limitations. If the user requires the use of a specific fused implementation,\n",
      "    disable the PyTorch C++ implementation using :func:`torch.backends.cuda.sdp_kernel`.\n",
      "    In the event that a fused implementation is not available, an error will be raised with the\n",
      "    reasons why the fused implementation cannot run.\n",
      "\n",
      "    Due to the nature of fusing floating point operations, the output of this function may be different\n",
      "    depending on what backend kernel is chosen.\n",
      "    The c++ implementation supports torch.float64 and can be used when higher precision is required.\n",
      "    For more information please see :doc:`/notes/numerical_accuracy`\n",
      "\n",
      "Note:\n",
      "    In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting ``torch.backends.cudnn.deterministic = True``. See :doc:`/notes/randomness` for more information.\n",
      "\n",
      "Args:\n",
      "    query (Tensor): Query tensor; shape :math:`(N, ..., L, E)`.\n",
      "    key (Tensor): Key tensor; shape :math:`(N, ..., S, E)`.\n",
      "    value (Tensor): Value tensor; shape :math:`(N, ..., S, Ev)`.\n",
      "    attn_mask (optional Tensor): Attention mask; shape :math:`(N, ..., L, S)`. Two types of masks are supported.\n",
      "        A boolean mask where a value of True indicates that the element *should* take part in attention.\n",
      "        A float mask of the same type as query, key, value that is added to the attention score.\n",
      "    dropout_p (float): Dropout probability; if greater than 0.0, dropout is applied\n",
      "    is_causal (bool): If true, assumes causal attention masking and errors if both attn_mask and is_causal\n",
      "        are set.\n",
      "    scale (optional float): Scaling factor applied prior to softmax. If None, the default value is set\n",
      "        to :math:`\\frac{1}{\\sqrt{E}}`.\n",
      "\n",
      "\n",
      "Returns:\n",
      "    output (Tensor): Attention output; shape :math:`(N, ..., L, Ev)`.\n",
      "\n",
      "Shape legend:\n",
      "    - :math:`N: \\text{Batch size} ... : \\text{Any number of other batch dimensions (optional)}`\n",
      "    - :math:`S: \\text{Source sequence length}`\n",
      "    - :math:`L: \\text{Target sequence length}`\n",
      "    - :math:`E: \\text{Embedding dimension of the query and key}`\n",
      "    - :math:`Ev: \\text{Embedding dimension of the value}`\n",
      "\n",
      "Examples::\n",
      "\n",
      "    >>> # Optionally use the context manager to ensure one of the fused kernels is run\n",
      "    >>> query = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n",
      "    >>> key = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n",
      "    >>> value = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n",
      "    >>> with torch.backends.cuda.sdp_kernel(enable_math=False):\n",
      "    >>>     F.scaled_dot_product_attention(query,key,value)\n",
      "\n",
      ".. _FlashAttention-2\\: Faster Attention with Better Parallelism and Work Partitioning:\n",
      "    https://arxiv.org/abs/2307.08691\n",
      ".. _Memory-Efficient Attention:\n",
      "    https://github.com/facebookresearch/xformers\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(scaled_dot_product_attention.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "57e06dbd-7333-477d-8696-105e8b464f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.fx as fx\n",
    "from torch.export import export, Dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d975ce2b-b389-4ffa-8402-bbfcab6bcf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_module = CausalSelfAttention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ff2af1ba-9a7d-4a16-bf3b-33157a9fddfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_x = torch.randint(0, 100, (1, att_module.config.block_size, att_module.config.n_embd), dtype=torch.long) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "02d0e728-d487-45e7-9405-674e1ba8c5c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_module.config.block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a195a98b-7e44-4004-a9d8-4603d942675d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dboyliao/Work/Sciwork/SciConf_2024/.venv/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:638: UserWarning: Was not able to add assertion to guarantee correct input x to specialized function. It is up to the user to make sure that your inputs match the inputs you specialized the function with.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "gm = fx.symbolic_trace(att_module, concrete_args={\"x\": sample_x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c90dd8da-9e31-40dc-bf06-d417b0a6a3d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.fx.graph_module.GraphModule.__new__.<locals>.GraphModuleImpl"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(gm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b7b8e413-fa86-43ab-96b3-6dad5d4dc93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_map = {}\n",
    "for node in gm.graph.nodes:\n",
    "    nodes_map[node.name] = node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2adaa2c8-14a4-42a1-9cbf-b1acccc3b0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name               target                             args                            kwargs\n",
      "-------------  -----------------  ---------------------------------  ------------------------------  ----------------------------------------------\n",
      "placeholder    x_1                x_1                                ()                              {}\n",
      "get_attr       _tensor_constant0  _tensor_constant0                  ()                              {}\n",
      "call_module    c_attn             c_attn                             (_tensor_constant0,)            {}\n",
      "call_method    split              split                              (c_attn, 768)                   {'dim': 2}\n",
      "call_function  getitem            <built-in function getitem>        (split, 0)                      {}\n",
      "call_function  getitem_1          <built-in function getitem>        (split, 1)                      {}\n",
      "call_function  getitem_2          <built-in function getitem>        (split, 2)                      {}\n",
      "call_method    view               view                               (getitem_1, 1, 1024, 12, 64)    {}\n",
      "call_method    transpose          transpose                          (view, 1, 2)                    {}\n",
      "call_method    view_1             view                               (getitem, 1, 1024, 12, 64)      {}\n",
      "call_method    transpose_1        transpose                          (view_1, 1, 2)                  {}\n",
      "call_method    view_2             view                               (getitem_2, 1, 1024, 12, 64)    {}\n",
      "call_method    transpose_2        transpose                          (view_2, 1, 2)                  {}\n",
      "call_method    transpose_3        transpose                          (transpose, -2, -1)             {}\n",
      "call_function  matmul             <built-in function matmul>         (transpose_1, transpose_3)      {}\n",
      "call_method    size               size                               (transpose, -1)                 {}\n",
      "call_function  sqrt               <built-in function sqrt>           (size,)                         {}\n",
      "call_function  truediv            <built-in function truediv>        (1.0, sqrt)                     {}\n",
      "call_function  mul                <built-in function mul>            (matmul, truediv)               {}\n",
      "get_attr       _tensor_constant1  _tensor_constant1                  ()                              {}\n",
      "call_method    masked_fill        masked_fill                        (mul, _tensor_constant1, -inf)  {}\n",
      "call_function  softmax            <function softmax at 0x110fddc60>  (masked_fill,)                  {'dim': -1, '_stacklevel': 3, 'dtype': None}\n",
      "call_function  dropout            <function dropout at 0x110fdd120>  (softmax,)                      {'p': 0.0, 'training': True, 'inplace': False}\n",
      "call_function  matmul_1           <built-in function matmul>         (dropout, transpose_2)          {}\n",
      "call_method    transpose_4        transpose                          (matmul_1, 1, 2)                {}\n",
      "call_method    contiguous         contiguous                         (transpose_4,)                  {}\n",
      "call_method    view_3             view                               (contiguous, 1, 1024, 768)      {}\n",
      "call_module    c_proj             c_proj                             (view_3,)                       {}\n",
      "call_module    resid_dropout      resid_dropout                      (c_proj,)                       {}\n",
      "output         output             output                             (resid_dropout,)                {}\n"
     ]
    }
   ],
   "source": [
    "gm.graph.print_tabular()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1773ddd4-be61-45f8-b680-8b11d43010aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('call_function',\n",
       " <function torch.nn.functional.dropout(input: torch.Tensor, p: float = 0.5, training: bool = True, inplace: bool = False) -> torch.Tensor>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes_map[\"dropout\"].op, nodes_map[\"dropout\"].target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ef88f63a-e9fc-440d-987b-6b612aae549d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replacement(q, k, v):\n",
    "    return scaled_dot_product_attention(\n",
    "        q, k, v, attn_mask=None, dropout_p=0.0, is_causal=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2bac99ea-6901-4b06-a9d0-70a34bb9d702",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatternModule(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                        .view(1, 1, config.block_size, config.block_size))\n",
    "    def forward(self, q, k, v):\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:1024,:1024] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = F.dropout(att, p=0.0)\n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(1, 1024, 768)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4b632837-e9a1-4f6b-9476-3e2ec24a006c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern(q, k, v):\n",
    "    pm = PatternModule(att_module.config)\n",
    "    return pm.forward(q, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d371b12d-4b03-4994-9f7e-6a90e806de36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Match(anchor=view, nodes_map={view: view_3, contiguous: contiguous, transpose_1: transpose_4, matmul_1: matmul_1, dropout: dropout, softmax: softmax, masked_fill: masked_fill, mul: mul, matmul: matmul, q: transpose_1, transpose: transpose_3, k: transpose, truediv: truediv, sqrt: sqrt, size: size, _tensor_constant0: _tensor_constant1, v: transpose_2})]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fx.replace_pattern(gm, pattern=pattern, replacement=replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2ad42823-1709-4afe-989e-c99f3aee0294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name                          target                                            args                                   kwargs\n",
      "-------------  ----------------------------  ------------------------------------------------  -------------------------------------  --------------------------------------------------------\n",
      "placeholder    x_1                           x_1                                               ()                                     {}\n",
      "get_attr       _tensor_constant0             _tensor_constant0                                 ()                                     {}\n",
      "call_module    c_attn                        c_attn                                            (_tensor_constant0,)                   {}\n",
      "call_method    split                         split                                             (c_attn, 768)                          {'dim': 2}\n",
      "call_function  getitem                       <built-in function getitem>                       (split, 0)                             {}\n",
      "call_function  getitem_1                     <built-in function getitem>                       (split, 1)                             {}\n",
      "call_function  getitem_2                     <built-in function getitem>                       (split, 2)                             {}\n",
      "call_method    view                          view                                              (getitem_1, 1, 1024, 12, 64)           {}\n",
      "call_method    transpose                     transpose                                         (view, 1, 2)                           {}\n",
      "call_method    view_1                        view                                              (getitem, 1, 1024, 12, 64)             {}\n",
      "call_method    transpose_1                   transpose                                         (view_1, 1, 2)                         {}\n",
      "call_method    view_2                        view                                              (getitem_2, 1, 1024, 12, 64)           {}\n",
      "call_method    transpose_2                   transpose                                         (view_2, 1, 2)                         {}\n",
      "call_function  scaled_dot_product_attention  <built-in function scaled_dot_product_attention>  (transpose_1, transpose, transpose_2)  {'attn_mask': None, 'dropout_p': 0.0, 'is_causal': True}\n",
      "call_module    c_proj                        c_proj                                            (scaled_dot_product_attention,)        {}\n",
      "call_module    resid_dropout                 resid_dropout                                     (c_proj,)                              {}\n",
      "output         output                        output                                            (resid_dropout,)                       {}\n"
     ]
    }
   ],
   "source": [
    "gm.graph.print_tabular()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803253b4-8189-44ea-aab1-50f05776ec30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sciconf-2024",
   "language": "python",
   "name": "sciconf-2024"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
